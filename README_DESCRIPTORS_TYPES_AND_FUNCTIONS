/*
 * main unit of I/O for the block layer and lower layers (ie drivers and
 * stacking drivers)
 */
/*
 * основновная единица ввода-вывода для блочного уровня и нижних уровней (т.е. драйверы и
 * стековые драйверы)
 */
// Stacking just means that some module calls functions defined in another module.                      // https://unix.stackexchange.com/questions/10812/module-stacking-in-linux-device-drivers
// Of course, all of them make calls to basic kernel features (which aren't in modules).
// Стекирование означает, что какой-то модуль вызывает функции, определенные в другом модуле. 
// Конечно, все они вызывают базовые функции ядра (которых нет в модулях).
/*
The central management structure (bio) is associated with a vector whose individual entries each point       // Wolfgang Mauerer "Professional Linux Kernel Architecture"
to a memory page (caution: Not the address in memory but the page instance belonging to the page).
These pages are used to receive data from and send data to the device.
The memory pages can but need not be organized contiguously; this facilitates the implementation of scatter-gather operations.
*/
/*
Центральная структура управления (био) связана с вектором, отдельные записи которого указывают     
на страницу памяти (внимание: не адрес в памяти, а экземпляр страницы, принадлежащий странице).
Эти страницы используются для получения данных с устройства и отправки данных на него.
Страницы памяти могут, но не обязательно, быть организованы последовательно; это облегчает реализацию операций разброса-сбора.
*/
struct bio {                                                                                           // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk_types.h#L211
	struct bio		*bi_next;	/* request queue link */  /* combines several BIOs in a singly linked list associated with a request */ 
	                                                                 /* объединяет несколько BIO в односвязный список, связанный с запросом */
	struct block_device	*bi_bdev;      /* is a pointer to the block device data structure of the device to which the request belongs */
	                                      /* указатель на структуру данных блочного устройства, к которому относится запрос */
	blk_opf_t		bi_opf;	/* bottom bits REQ_OP, top bits   // typedef __u32 __bitwise blk_opf_t; in https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk_types.h#L202
						 * req_flags.
						 */
	unsigned short		bi_flags;	/* BIO_* below */
	unsigned short		bi_ioprio;
	enum rw_hint		bi_write_hint;
	blk_status_t		bi_status;               // typedef u8 __bitwise blk_status_t; in https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk_types.h#L93
	atomic_t		__bi_remaining;

	struct bvec_iter	bi_iter;

	union {
		/* for polled bios: */    /* для опрашиваемых биосов: */
		blk_qc_t		bi_cookie;       // typedef unsigned int blk_qc_t; in  https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk_types.h#L204
		/* for plugged zoned writes only: */    /* только для подключенных зонных записей: */
		unsigned int		__bi_nr_segments;
	};
	/*
	bio_end_io_t must be invoked by the device driver when hardware transfer is completed. This           // Wolfgang Mauerer "Professional Linux Kernel Architecture"
        gives the block layer the opportunity to do clean-up work or wake sleeping processes that are
        waiting for the request to end.
        */
        /*
        bio_end_io_t должен быть вызван драйвером устройства после завершения передачи оборудования. Это
        дает блочному слою возможность выполнить очистку или разбудить спящие процессы, которые ожидают завершения запроса.
        */
	bio_end_io_t		*bi_end_io;            // typedef void (bio_end_io_t) (struct bio *); in https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk_types.h#L21   
	void			*bi_private;                                    // is not modified by the generic BIO code and can be used for driver-specific information.
	                                                                       // не модифицируется общим кодом BIO и может использоваться для получения информации, специфичной для драйвера.
#ifdef CONFIG_BLK_CGROUP
	/*
	 * Represents the association of the css and request_queue for the bio.
	 * If a bio goes direct to device, it will not have a blkg as it will
	 * not have a request_queue associated with it.  The reference is put
	 * on release of the bio.
	 */
	/*
	 * Представляет связь css и request_queue для bio.
	 * Если bio отправляется напрямую на устройство, она не будет иметь blkg, поскольку она
	 * не будет иметь request_queue, связанного с ней. Ссылка помещается
	 * при выпуске bio.
	 */
	struct blkcg_gq		*bi_blkg;
	struct bio_issue	bi_issue;
#ifdef CONFIG_BLK_CGROUP_IOCOST
	u64			bi_iocost_cost;
#endif
#endif

#ifdef CONFIG_BLK_INLINE_ENCRYPTION
	struct bio_crypt_ctx	*bi_crypt_context;
#endif

	union {
#if defined(CONFIG_BLK_DEV_INTEGRITY)
		struct bio_integrity_payload *bi_integrity; /* data integrity */
#endif
	};

	unsigned short		bi_vcnt;	/* how many bio_vec's */    /* сколько bio_vec */

	/*
	 * Everything starting with bi_max_vecs will be preserved by bio_reset()
	 */

	unsigned short		bi_max_vecs;	/* max bvl_vecs we can hold */    /* максимальное количество bvl_vecs, которое мы можем удерживать */

	atomic_t		__bi_cnt;	/* pin count */

	struct bio_vec		*bi_io_vec;	/* the actual vec list */        /* фактический список векторов */

	struct bio_set		*bi_pool;

	/*
	 * We can inline a number of vecs at the end of the bio, to avoid
	 * double allocations for a small number of bio_vecs. This member
	 * MUST obviously be kept at the very end of the bio.
	 */
	/*
	 * Мы можем встроить несколько векторов в конец bio, чтобы избежать
	 * двойного выделения памяти для небольшого количества bio_vecs. Этот член
	 * ДОЛЖЕН, очевидно, храниться в самом конце bio.
	 */
	struct bio_vec		bi_inline_vecs[];
};

-----------------------------------------------------------------------------------------------------------------------------------------------------------
/* Block storage write lifetime hint values. */   /* Значения подсказок для записи времени жизни блочного хранилища. */ 
enum rw_hint {                                                // https://elixir.bootlin.com/linux/v6.11/source/include/linux/rw_hint.h#L10
	WRITE_LIFE_NOT_SET	= RWH_WRITE_LIFE_NOT_SET,
	WRITE_LIFE_NONE		= RWH_WRITE_LIFE_NONE,
	WRITE_LIFE_SHORT	= RWH_WRITE_LIFE_SHORT,
	WRITE_LIFE_MEDIUM	= RWH_WRITE_LIFE_MEDIUM,
	WRITE_LIFE_LONG		= RWH_WRITE_LIFE_LONG,
	WRITE_LIFE_EXTREME	= RWH_WRITE_LIFE_EXTREME,
} __packed;

-----------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * bio_set is used to allow other portions of the IO system to
 * allocate their own private memory pools for bio and iovec structures.
 * These memory pools in turn all allocate from the bio_slab
 * and the bvec_slabs[].
 */
/*
* bio_set используется для того, чтобы другие части системы ввода-вывода могли
* выделять свои собственные пулы памяти для структур bio и iovec.
* Эти пулы памяти, в свою очередь, выделяются из bio_slab
* и bvec_slabs[].
*/

#define BIO_POOL_SIZE 2

struct bio_set {                                                         // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bio.h#L624
	struct kmem_cache *bio_slab;
	unsigned int front_pad;

	/*
	 * per-cpu bio alloc cache
	 */
	struct bio_alloc_cache __percpu *cache;

	mempool_t bio_pool;
	mempool_t bvec_pool;
#if defined(CONFIG_BLK_DEV_INTEGRITY)
	mempool_t bio_integrity_pool;
	mempool_t bvec_integrity_pool;
#endif

	unsigned int back_pad;
	/*
	 * Deadlock avoidance for stacking block drivers: see comments in
	 * bio_alloc_bioset() for details
	 */
	spinlock_t		rescue_lock;
	struct bio_list		rescue_list;
	struct work_struct	rescue_work;
	struct workqueue_struct	*rescue_workqueue;

	/*
	 * Hot un-plug notifier for the per-cpu cache, if used
	 */
	struct hlist_node cpuhp_dead;
};

-----------------------------------------------------------------------------------------------------------------------------------------------------------
struct block_device {                                                                 // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk_types.h#L41
	sector_t		bd_start_sect;                                 // typedef u64 sector_t;  in https://elixir.bootlin.com/linux/v6.11/source/include/linux/types.h#L134
	sector_t		bd_nr_sectors;
	struct gendisk *	bd_disk;
	struct request_queue *	bd_queue;
	struct disk_stats __percpu *bd_stats;
	unsigned long		bd_stamp;
	atomic_t		__bd_flags;	// partition number + flags    // номер раздела + флаги
#define BD_PARTNO		255	// lower 8 bits; assign-once           // нижние 8 бит; присваиваем один раз
#define BD_READ_ONLY		(1u<<8) // read-only policy                    // политика только для чтения
#define BD_WRITE_HOLDER		(1u<<9)
#define BD_HAS_SUBMIT_BIO	(1u<<10)
#define BD_RO_WARNED		(1u<<11)
#ifdef CONFIG_FAIL_MAKE_REQUEST
#define BD_MAKE_IT_FAIL		(1u<<12)
#endif
	dev_t			bd_dev;
	struct address_space	*bd_mapping;	/* page cache */     /* кэш страницы */

	atomic_t		bd_openers;
	spinlock_t		bd_size_lock; /* for bd_inode->i_size updates */
	void *			bd_claiming;
	void *			bd_holder;
	const struct blk_holder_ops *bd_holder_ops;
	struct mutex		bd_holder_lock;
	int			bd_holders;
	struct kobject		*bd_holder_dir;

	atomic_t		bd_fsfreeze_count; /* number of freeze requests */   /* количество запросов на заморозку */
	struct mutex		bd_fsfreeze_mutex; /* serialize freeze/thaw */       /* сериализация заморозки/разморозки */

	struct partition_meta_info *bd_meta_info;
	int			bd_writers;
	/*
	 * keep this out-of-line as it's both big and not needed in the fast path
	 */
	/*
	 * держите это отдельно, так как это и большое, и не нужно в быстром пути
	 */
	struct device		bd_device;
} __randomize_layout;
-----------------------------------------------------------------------------------------------------------------------------------------------------------
/**
 * struct bio_vec - a contiguous range of physical memory addresses
 * @bv_page:   First page associated with the address range.
 * @bv_len:    Number of bytes in the address range.
 * @bv_offset: Start of the address range relative to the start of @bv_page.
 *
 * The following holds for a bvec if n * PAGE_SIZE < bv_offset + bv_len:
 *
 *   nth_page(@bv_page, n) == @bv_page + n
 *
 * This holds because page_is_mergeable() checks the above property.
 */
/**
 * struct bio_vec — непрерывный диапазон адресов физической памяти
 * @bv_page: Первая страница, связанная с диапазоном адресов.
 * @bv_len: Количество байтов в диапазоне адресов.
 * @bv_offset: Начало диапазона адресов относительно начала @bv_page.
 *
 * Следующее справедливо для bvec, если n * PAGE_SIZE < bv_offset + bv_len:
 *
 * nth_page(@bv_page, n) == @bv_page + n
 *
 * Это справедливо, поскольку page_is_mergeable() проверяет указанное выше свойство.
 */
/*
bv_page points to the page instance of the page used for data transfer. bv_offset indicates the     // Wolfgang Mauerer "Professional Linux Kernel Architecture"
offset within the page; typically this value is 0 because page boundaries are normally used as
boundaries for I/O operations.
len specifies the number of bytes used for the data if the whole page is not filled.
*/
/*
bv_page указывает на экземпляр страницы, используемой для передачи данных (с устройства и на устройство). bv_offset указывает смещение внутри страницы; обычно это значение равно 0, 
поскольку границы страницы обычно используются как границы для операций ввода-вывода.
len указывает количество байтов, используемых для данных, если вся страница не заполнена.
*/
struct bio_vec {                                                               // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L31
	struct page	*bv_page;
	unsigned int	bv_len;
	unsigned int	bv_offset;
};

------------------------------------------------------------------------------------------------------------------------------------------------------------
struct page {                                                                  //  https://elixir.bootlin.com/linux/v6.11/source/include/linux/mm_types.h#L72
	unsigned long flags;		/* Atomic flags, some possibly
					 * updated asynchronously */
	/*
	 * Five words (20/40 bytes) are available in this union.
	 * WARNING: bit 0 of the first word is used for PageTail(). That
	 * means the other users of this union MUST NOT use the bit to
	 * avoid collision and false-positive PageTail().
	 */
	union {
		struct {	/* Page cache and anonymous pages */
			/**
			 * @lru: Pageout list, eg. active_list protected by
			 * lruvec->lru_lock.  Sometimes used as a generic list
			 * by the page owner.
			 */
			union {
				struct list_head lru;

				/* Or, for the Unevictable "LRU list" slot */
				struct {
					/* Always even, to negate PageTail */
					void *__filler;
					/* Count page's or folio's mlocks */
					unsigned int mlock_count;
				};

				/* Or, free page */
				struct list_head buddy_list;
				struct list_head pcp_list;
			};
			/* See page-flags.h for PAGE_MAPPING_FLAGS */
			struct address_space *mapping;
			union {
				pgoff_t index;		/* Our offset within mapping. */
				unsigned long share;	/* share count for fsdax */
			};
			/**
			 * @private: Mapping-private opaque data.
			 * Usually used for buffer_heads if PagePrivate.
			 * Used for swp_entry_t if PageSwapCache.
			 * Indicates order in the buddy system if PageBuddy.
			 */
			unsigned long private;
		};
		struct {	/* page_pool used by netstack */
			/**
			 * @pp_magic: magic value to avoid recycling non
			 * page_pool allocated pages.
			 */
			unsigned long pp_magic;
			struct page_pool *pp;
			unsigned long _pp_mapping_pad;
			unsigned long dma_addr;
			atomic_long_t pp_ref_count;
		};
		struct {	/* Tail pages of compound page */
			unsigned long compound_head;	/* Bit zero is set */
		};
		struct {	/* ZONE_DEVICE pages */
			/** @pgmap: Points to the hosting device page map. */
			struct dev_pagemap *pgmap;
			void *zone_device_data;
			/*
			 * ZONE_DEVICE private pages are counted as being
			 * mapped so the next 3 words hold the mapping, index,
			 * and private fields from the source anonymous or
			 * page cache page while the page is migrated to device
			 * private memory.
			 * ZONE_DEVICE MEMORY_DEVICE_FS_DAX pages also
			 * use the mapping, index, and private fields when
			 * pmem backed DAX files are mapped.
			 */
		};

		/** @rcu_head: You can use this to free a page by RCU. */
		struct rcu_head rcu_head;
	};

	union {		/* This union is 4 bytes in size. */
		/*
		 * For head pages of typed folios, the value stored here
		 * allows for determining what this page is used for. The
		 * tail pages of typed folios will not store a type
		 * (page_type == _mapcount == -1).
		 *
		 * See page-flags.h for a list of page types which are currently
		 * stored here.
		 *
		 * Owners of typed folios may reuse the lower 16 bit of the
		 * head page page_type field after setting the page type,
		 * but must reset these 16 bit to -1 before clearing the
		 * page type.
		 */
		unsigned int page_type;

		/*
		 * For pages that are part of non-typed folios for which mappings
		 * are tracked via the RMAP, encodes the number of times this page
		 * is directly referenced by a page table.
		 *
		 * Note that the mapcount is always initialized to -1, so that
		 * transitions both from it and to it can be tracked, using
		 * atomic_inc_and_test() and atomic_add_negative(-1).
		 */
		atomic_t _mapcount;
	};

	/* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */
	atomic_t _refcount;

#ifdef CONFIG_MEMCG
	unsigned long memcg_data;
#elif defined(CONFIG_SLAB_OBJ_EXT)
	unsigned long _unused_slab_obj_exts;
#endif

	/*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
#if defined(WANT_PAGE_VIRTUAL)
	void *virtual;			/* Kernel virtual address (NULL if
					   not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */

#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
	int _last_cpupid;
#endif

#ifdef CONFIG_KMSAN
	/*
	 * KMSAN metadata for this page:
	 *  - shadow page: every bit indicates whether the corresponding
	 *    bit of the original page is initialized (0) or not (1);
	 *  - origin page: every 4 bytes contain an id of the stack trace
	 *    where the uninitialized value was created.
	 */
	struct page *kmsan_shadow;
	struct page *kmsan_origin;
#endif
} _struct_page_alignment;

--------------------------------------------------------------------------------------------------------------------------------------------------
/**
 * page_address - get the mapped virtual address of a page
 * @page: &struct page to get the virtual address of
 *
 * Returns the page's virtual address.
 */
/**
 * page_address - получить сопоставленный виртуальный адрес страницы
 * @page: &struct page для получения виртуального адреса
 *
 * Возвращает виртуальный адрес страницы.
 */
void *page_address(const struct page *page)                             //  https://elixir.bootlin.com/linux/v6.11/source/mm/highmem.c#L753
{
	unsigned long flags;
	void *ret;
	struct page_address_slot *pas;

	if (!PageHighMem(page))
		return lowmem_page_address(page);

	pas = page_slot(page);
	ret = NULL;
	spin_lock_irqsave(&pas->lock, flags);
	if (!list_empty(&pas->lh)) {
		struct page_address_map *pam;

		list_for_each_entry(pam, &pas->lh, list) {
			if (pam->page == page) {
				ret = pam->virtual;
				break;
			}
		}
	}

	spin_unlock_irqrestore(&pas->lock, flags);
	return ret;
}

------------------------------------------------------------------------------------------------------------------------------------------------------------
struct bvec_iter {                                                                          // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L77
	sector_t		bi_sector;	/* device address in 512 byte sectors */          /* адрес устройства в секторах по 512 байт */
	unsigned int		bi_size;	/* residual I/O count */                          /* остаточное количество операций ввода-вывода */

	unsigned int		bi_idx;		/* current index into bvl_vec */          /* текущий индекс в bvl_vec */

	unsigned int            bi_bvec_done;	/* number of bytes completed in current bvec */   /* количество байтов, завершенных в текущем bvec */
} __packed __aligned(4);

------------------------------------------------------------------------------------------------------------------------------------------------------------
struct gendisk {                                                                           // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blkdev.h#L141
	/*
	 * major/first_minor/minors should not be set by any new driver, the
	 * block core will take care of allocating them automatically.
	 */
	/*
	 * major/first_minor/minors не должны устанавливаться никаким новым драйвером,
	 * ядро ​​блока позаботится об их выделении автоматически.
	 */
	int major;
	int first_minor;
	int minors;

	char disk_name[DISK_NAME_LEN];	/* name of major driver */              /* имя основного драйвера */

	unsigned short events;		/* supported events */                         /* поддерживаемые события */
	unsigned short event_flags;	/* flags related to event processing */        /* флаги, связанные с обработкой событий */

	struct xarray part_tbl;
	struct block_device *part0;

	const struct block_device_operations *fops;
	struct request_queue *queue;
	void *private_data;

	struct bio_set bio_split;

	int flags;
	unsigned long state;
#define GD_NEED_PART_SCAN		0
#define GD_READ_ONLY			1
#define GD_DEAD				2
#define GD_NATIVE_CAPACITY		3
#define GD_ADDED			4
#define GD_SUPPRESS_PART_SCAN		5
#define GD_OWNS_QUEUE			6

	struct mutex open_mutex;	/* open/close mutex */                         /* открыть/закрыть мьютекс */
	unsigned open_partitions;	/* number of open partitions */                /* количество открытых разделов */

	struct backing_dev_info	*bdi;
	struct kobject queue_kobj;	/* the queue/ directory */                     /* очередь/ каталог */
	struct kobject *slave_dir;
#ifdef CONFIG_BLOCK_HOLDER_DEPRECATED
	struct list_head slave_bdevs;
#endif
	struct timer_rand_state *random;
	atomic_t sync_io;		/* RAID */
	struct disk_events *ev;

#ifdef CONFIG_BLK_DEV_ZONED
	/*
	 * Zoned block device information. Reads of this information must be
	 * protected with blk_queue_enter() / blk_queue_exit(). Modifying this
	 * information is only allowed while no requests are being processed.
	 * See also blk_mq_freeze_queue() and blk_mq_unfreeze_queue().
	 */
	/*
	 * Информация о зонированном блочном устройстве. Чтение этой информации должно быть
	 * защищено с помощью blk_queue_enter() / blk_queue_exit(). Изменение этой
	 * информации разрешено только при отсутствии обрабатываемых запросов.
	 * См. также blk_mq_freeze_queue() и blk_mq_unfreeze_queue().
	 */
	unsigned int		nr_zones;
	unsigned int		zone_capacity;
	unsigned int		last_zone_capacity;
	unsigned long		*conv_zones_bitmap;
	unsigned int            zone_wplugs_hash_bits;
	spinlock_t              zone_wplugs_lock;
	struct mempool_s	*zone_wplugs_pool;
	struct hlist_head       *zone_wplugs_hash;
	struct list_head        zone_wplugs_err_list;
	struct work_struct	zone_wplugs_work;
	struct workqueue_struct *zone_wplugs_wq;
#endif /* CONFIG_BLK_DEV_ZONED */

#if IS_ENABLED(CONFIG_CDROM)
	struct cdrom_device_info *cdi;
#endif
	int node_id;
	struct badblocks *bb;
	struct lockdep_map lockdep_map;
	u64 diskseq;
	blk_mode_t open_mode;

	/*
	 * Independent sector access ranges. This is always NULL for
	 * devices that do not have multiple independent access ranges.
	 */
	/*
	 * Независимые диапазоны доступа к секторам. Это всегда NULL для
	 * устройств, которые не имеют нескольких независимых диапазонов доступа.
	 */
	struct blk_independent_access_ranges *ia_ranges;
};

------------------------------------------------------------------------------------------------------------------------------------------------------------
struct block_device_operations {                                                            // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blkdev.h#L1470
	void (*submit_bio)(struct bio *bio);
	int (*poll_bio)(struct bio *bio, struct io_comp_batch *iob,
			unsigned int flags);
	int (*open)(struct gendisk *disk, blk_mode_t mode);
	void (*release)(struct gendisk *disk);
	int (*ioctl)(struct block_device *bdev, blk_mode_t mode,
			unsigned cmd, unsigned long arg);
	int (*compat_ioctl)(struct block_device *bdev, blk_mode_t mode,
			unsigned cmd, unsigned long arg);
	unsigned int (*check_events) (struct gendisk *disk,
				      unsigned int clearing);
	void (*unlock_native_capacity) (struct gendisk *);
	int (*getgeo)(struct block_device *, struct hd_geometry *);
	int (*set_read_only)(struct block_device *bdev, bool ro);
	void (*free_disk)(struct gendisk *disk);
	/* this callback is with swap_lock and sometimes page table lock held */       /* этот обратный вызов выполняется с блокировкой swap_lock и иногда с блокировкой таблицы страниц */
	void (*swap_slot_free_notify) (struct block_device *, unsigned long);
	int (*report_zones)(struct gendisk *, sector_t sector,
			unsigned int nr_zones, report_zones_cb cb, void *data);
	char *(*devnode)(struct gendisk *disk, umode_t *mode);
	/* returns the length of the identifier or a negative errno: */                /* возвращает длину идентификатора или отрицательное значение errno: */
	int (*get_unique_id)(struct gendisk *disk, u8 id[16],
			enum blk_unique_id id_type);
	struct module *owner;
	const struct pr_ops *pr_ops;

	/*
	 * Special callback for probing GPT entry at a given sector.
	 * Needed by Android devices, used by GPT scanner and MMC blk
	 * driver.
	 */
	/*
	 * Специальный обратный вызов для проверки записи GPT в заданном секторе.
	 * Требуется для устройств Android, используется сканером GPT и драйвером MMC blk.
	 */
	int (*alternative_gpt_sector)(struct gendisk *disk, sector_t *sector);
};

------------------------------------------------------------------------------------------------------------------------------------------------------------
/**
 * struct blk_mq_tag_set - tag set that can be shared between request queues            
 * @ops:	   Pointers to functions that implement block driver behavior.
 * @map:	   One or more ctx -> hctx mappings. One map exists for each
 *		   hardware queue type (enum hctx_type) that the driver wishes
 *		   to support. There are no restrictions on maps being of the
 *		   same size, and it's perfectly legal to share maps between
 *		   types.
 * @nr_maps:	   Number of elements in the @map array. A number in the range
 *		   [1, HCTX_MAX_TYPES].
 * @nr_hw_queues:  Number of hardware queues supported by the block driver that
 *		   owns this data structure.
 * @queue_depth:   Number of tags per hardware queue, reserved tags included.
 * @reserved_tags: Number of tags to set aside for BLK_MQ_REQ_RESERVED tag
 *		   allocations.
 * @cmd_size:	   Number of additional bytes to allocate per request. The block
 *		   driver owns these additional bytes.
 * @numa_node:	   NUMA node the storage adapter has been connected to.
 * @timeout:	   Request processing timeout in jiffies.
 * @flags:	   Zero or more BLK_MQ_F_* flags.
 * @driver_data:   Pointer to data owned by the block driver that created this
 *		   tag set.
 * @tags:	   Tag sets. One tag set per hardware queue. Has @nr_hw_queues
 *		   elements.
 * @shared_tags:
 *		   Shared set of tags. Has @nr_hw_queues elements. If set,
 *		   shared by all @tags.
 * @tag_list_lock: Serializes tag_list accesses.
 * @tag_list:	   List of the request queues that use this tag set. See also
 *		   request_queue.tag_set_list.
 * @srcu:	   Use as lock when type of the request queue is blocking
 *		   (BLK_MQ_F_BLOCKING).
 */
 /**
* struct blk_mq_tag_set — набор тегов, который может быть общим для очередей запросов
* @ops: Указатели на функции, реализующие поведение блочного драйвера.
* @map: Одно или несколько отображений ctx -> hctx. Существует одно отображение для каждого
* типа аппаратной очереди (enum hctx_type), который драйвер хочет
* поддерживать. Нет ограничений на то, чтобы карты были
* одинакового размера, и совершенно допустимо совместное использование карт между
* типами.
* @nr_maps: Количество элементов в массиве @map. Число в диапазоне
* [1, HCTX_MAX_TYPES].
* @nr_hw_queues: Количество аппаратных очередей, поддерживаемых блочным драйвером, которому
* принадлежит эта структура данных.
* @queue_depth: Количество тегов на аппаратную очередь, включая зарезервированные теги.
* @reserved_tags: Количество тегов, отложенных для тега BLK_MQ_REQ_RESERVED
* выделений.
* @cmd_size: Количество дополнительных байтов для выделения на запрос. Драйвер блока
* владеет этими дополнительными байтами.
* @numa_node: Узел NUMA, к которому подключен адаптер хранилища.
* @timeout: Тайм-аут обработки запроса в мизерах.
* @flags: Ноль или более флагов BLK_MQ_F_*.
* @driver_data: Указатель на данные, принадлежащие драйверу блока, который создал этот
* набор тегов.
* @tags: Наборы тегов. Один набор тегов на аппаратную очередь. Имеет элементы @nr_hw_queues
*.
* @shared_tags:
* Общий набор тегов. Имеет элементы @nr_hw_queues. Если установлено,
* используется всеми @tags.

* @tag_list_lock: Сериализует доступы tag_list.
* @tag_list: Список очередей запросов, которые используют этот набор тегов. См. также
* request_queue.tag_set_list.
* @srcu: Использовать как блокировку, когда тип очереди запросов — блокирующий
* (BLK_MQ_F_BLOCKING).
*/
struct blk_mq_tag_set {                                                                      //  https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L522
	const struct blk_mq_ops	*ops;
	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
	unsigned int		nr_maps;
	unsigned int		nr_hw_queues;
	unsigned int		queue_depth;
	unsigned int		reserved_tags;
	unsigned int		cmd_size;
	int			numa_node;
	unsigned int		timeout;
	unsigned int		flags;
	void			*driver_data;

	struct blk_mq_tags	**tags;

	struct blk_mq_tags	*shared_tags;

	struct mutex		tag_list_lock;
	struct list_head	tag_list;
	struct srcu_struct	*srcu;
};

----------------------------------------------------------------------------------------------------------------------------------
/**
 * struct blk_mq_ops - Callback functions that implements block driver behaviour.
 */
/**
* struct blk_mq_ops - Функции обратного вызова, реализующие поведение драйвера блока.
*/
struct blk_mq_ops {                                                                              // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L561
	/**
	 * @queue_rq: Queue a new request from block IO.
	 */
	/**
	 * @queue_rq: Поставить в очередь новый запрос из блока ввода-вывода.
	 */
	blk_status_t (*queue_rq)(struct blk_mq_hw_ctx *,
				 const struct blk_mq_queue_data *);

	/**
	 * @commit_rqs: If a driver uses bd->last to judge when to submit
	 * requests to hardware, it must define this function. In case of errors
	 * that make us stop issuing further requests, this hook serves the
	 * purpose of kicking the hardware (which the last request otherwise
	 * would have done).
	 */
	/**
	 * @commit_rqs: Если драйвер использует bd->last для оценки того, когда отправлять
	 * запросы оборудованию, он должен определить эту функцию. В случае ошибок,
	 * которые заставляют нас прекратить отправку дальнейших запросов, этот хук служит
	 * цели отключения оборудования (что в противном случае сделал бы последний запрос).
	 */
	void (*commit_rqs)(struct blk_mq_hw_ctx *);

	/**
	 * @queue_rqs: Queue a list of new requests. Driver is guaranteed
	 * that each request belongs to the same queue. If the driver doesn't
	 * empty the @rqlist completely, then the rest will be queued
	 * individually by the block layer upon return.
	 */
	/**
	 * @queue_rqs: Поставить в очередь список новых запросов. Драйверу гарантируется,
	 * что каждый запрос принадлежит одной и той же очереди. Если драйвер не
	 * полностью очистит @rqlist, то остальные будут поставлены в очередь
	 * индивидуально блочным слоем по возвращении.
	 */
	void (*queue_rqs)(struct request **rqlist);

	/**
	 * @get_budget: Reserve budget before queue request, once .queue_rq is
	 * run, it is driver's responsibility to release the
	 * reserved budget. Also we have to handle failure case
	 * of .get_budget for avoiding I/O deadlock.
	 */
	/**
	 * @get_budget: Резервный бюджет перед запросом очереди, после того, как .queue_rq
	 * запущен, драйвер несет ответственность за освобождение
	 * зарезервированного бюджета. Также мы должны обработать случай сбоя
	 * .get_budget, чтобы избежать взаимоблокировки ввода-вывода.
	 */
	int (*get_budget)(struct request_queue *);

	/**
	 * @put_budget: Release the reserved budget.
	 */
	/**
	 * @put_budget: Освободить зарезервированный бюджет.
	 */
	void (*put_budget)(struct request_queue *, int);

	/**
	 * @set_rq_budget_token: store rq's budget token
	 */
	/**
	 * @set_rq_budget_token: сохранить токен бюджета rq
	 */
	void (*set_rq_budget_token)(struct request *, int);
	
	/**
	 * @get_rq_budget_token: retrieve rq's budget token
	 */
	/**
	 * @get_rq_budget_token: получить токен бюджета rq
	 */
	int (*get_rq_budget_token)(struct request *);

	/**
	 * @timeout: Called on request timeout.
	 */
	/**
	 * @timeout: Вызывается по тайм-ауту запроса.
	 */
	enum blk_eh_timer_return (*timeout)(struct request *);

	/**
	 * @poll: Called to poll for completion of a specific tag.
	 */
	/**
	 * @poll: Вызывается для опроса на предмет завершения определенного тега.
	 */
	int (*poll)(struct blk_mq_hw_ctx *, struct io_comp_batch *);

	/**
	 * @complete: Mark the request as complete.
	 */
	/**
	 * @complete: Отметить запрос как выполненный.
	 */
	void (*complete)(struct request *);

	/**
	 * @init_hctx: Called when the block layer side of a hardware queue has
	 * been set up, allowing the driver to allocate/init matching
	 * structures.
	 */
	/**
	 * @init_hctx: Вызывается, когда сторона блочного уровня аппаратной очереди
	 * настроена, позволяя драйверу выделять/инициализировать соответствующие
	 * структуры.
	 */
	int (*init_hctx)(struct blk_mq_hw_ctx *, void *, unsigned int);
	
	/**
	 * @exit_hctx: Ditto for exit/teardown.
	 */
	/**
	 * @exit_hctx: То же самое для выхода/демонтажа.
	 */
	void (*exit_hctx)(struct blk_mq_hw_ctx *, unsigned int);

	/**
	 * @init_request: Called for every command allocated by the block layer
	 * to allow the driver to set up driver specific data.
	 *
	 * Tag greater than or equal to queue_depth is for setting up
	 * flush request.
	 */
	/**
	 * @init_request: Вызывается для каждой команды, выделенной блочным слоем,
	 * чтобы позволить драйверу настроить специфические данные драйвера.
	 *
	 * Тег, больший или равный queue_depth, предназначен для настройки
	 * запроса на сброс.
	 */
	int (*init_request)(struct blk_mq_tag_set *set, struct request *,
			    unsigned int, unsigned int);
	/**
	 * @exit_request: Ditto for exit/teardown.
	 */
	/**
	 * @exit_request: То же самое для выхода/демонтажа.
	 */
	void (*exit_request)(struct blk_mq_tag_set *set, struct request *,
			     unsigned int);

	/**
	 * @cleanup_rq: Called before freeing one request which isn't completed
	 * yet, and usually for freeing the driver private data.
	 */
	/**
	 * @cleanup_rq: Вызывается перед освобождением одного запроса, который еще не завершен,
	 * и обычно для освобождения личных данных драйвера.
	 */
	void (*cleanup_rq)(struct request *);

	/**
	 * @busy: If set, returns whether or not this queue currently is busy.
	 */
	/**
	 * @busy: Если установлено, возвращает, занята ли эта очередь в данный момент.
	 */
	bool (*busy)(struct request_queue *);

	/**
	 * @map_queues: This allows drivers specify their own queue mapping by
	 * overriding the setup-time function that builds the mq_map.
	 */
	/**
	 * @map_queues: Это позволяет драйверам указывать собственное отображение очередей,
	 * переопределяя функцию времени настройки, которая создает mq_map.
	 */
	void (*map_queues)(struct blk_mq_tag_set *set);

#ifdef CONFIG_BLK_DEBUG_FS
	/**
	 * @show_rq: Used by the debugfs implementation to show driver-specific
	 * information about a request.
	 */
	/**
	 * @show_rq: Используется реализацией debugfs для отображения специфичной для драйвера
	 * информации о запросе.
	 */
	void (*show_rq)(struct seq_file *m, struct request *rq);
#endif
};

---------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * Try to put the fields that are referenced together in the same cacheline.
 *
 * If you modify this structure, make sure to update blk_rq_init() and
 * especially blk_mq_rq_ctx_init() to take care of the added fields.
 */
/*
* Попробуйте поместить поля, на которые есть ссылки, вместе в одну строку кэша.
*
* Если вы измените эту структуру, обязательно обновите blk_rq_init() и особенно blk_mq_rq_ctx_init(), чтобы позаботиться о добавленных полях.
*/
struct request {                                                              // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L102
	struct request_queue *q;
	struct blk_mq_ctx *mq_ctx;
	struct blk_mq_hw_ctx *mq_hctx;

	blk_opf_t cmd_flags;		/* op and common flags */
	req_flags_t rq_flags;

	int tag;
	int internal_tag;

	unsigned int timeout;

	/* the following two fields are internal, NEVER access directly */
	unsigned int __data_len;	/* total data len */
	sector_t __sector;		/* sector cursor */   /* курсор сектора */

	struct bio *bio;
	struct bio *biotail;

	union {
		struct list_head queuelist;
		struct request *rq_next;
	};

	struct block_device *part;
#ifdef CONFIG_BLK_RQ_ALLOC_TIME
	/* Time that the first bio started allocating this request. */
	u64 alloc_time_ns;
#endif
	/* Time that this request was allocated for this IO. */
	u64 start_time_ns;
	/* Time that I/O was submitted to the device. */
	u64 io_start_time_ns;

#ifdef CONFIG_BLK_WBT
	unsigned short wbt_flags;
#endif
	/*
	 * rq sectors used for blk stats. It has the same value
	 * with blk_rq_sectors(rq), except that it never be zeroed
	 * by completion.
	 */
	unsigned short stats_sectors;

	/*
	 * Number of scatter-gather DMA addr+len pairs after
	 * physical address coalescing is performed.
	 */
	unsigned short nr_phys_segments;

#ifdef CONFIG_BLK_DEV_INTEGRITY
	unsigned short nr_integrity_segments;
#endif

#ifdef CONFIG_BLK_INLINE_ENCRYPTION
	struct bio_crypt_ctx *crypt_ctx;
	struct blk_crypto_keyslot *crypt_keyslot;
#endif

	enum rw_hint write_hint;
	unsigned short ioprio;

	enum mq_rq_state state;
	atomic_t ref;

	unsigned long deadline;

	/*
	 * The hash is used inside the scheduler, and killed once the
	 * request reaches the dispatch list. The ipi_list is only used
	 * to queue the request for softirq completion, which is long
	 * after the request has been unhashed (and even removed from
	 * the dispatch list).
	 */
	union {
		struct hlist_node hash;	/* merge hash */
		struct llist_node ipi_list;
	};

	/*
	 * The rb_node is only used inside the io scheduler, requests
	 * are pruned when moved to the dispatch queue. special_vec must
	 * only be used if RQF_SPECIAL_PAYLOAD is set, and those cannot be
	 * insert into an IO scheduler.
	 */
	union {
		struct rb_node rb_node;	/* sort/lookup */
		struct bio_vec special_vec;
	};

	/*
	 * Three pointers are available for the IO schedulers, if they need
	 * more they have to dynamically allocate it.
	 */
	struct {
		struct io_cq		*icq;
		void			*priv[2];
	} elv;

	struct {
		unsigned int		seq;
		rq_end_io_fn		*saved_end_io;
	} flush;

	u64 fifo_time;

	/*
	 * completion callback.
	 */
	rq_end_io_fn *end_io;
	void *end_io_data;
};

------------------------------------------------------------------------------------------------------------------------------
struct list_head {                                                    // https://elixir.bootlin.com/linux/v6.11/source/include/linux/types.h#L193
	struct list_head *next, *prev;
};
/*
Специальная структура, реализующая двунаправленные списки, два поля которой, next и prev, содержат указатели вперед и назад для элемента двунаправленного списка общего назначения. 
При этом важно отметить, что указатели в полях list_head содержат адреса других полей list_head, а не адреса структур, включающих в себя структуру list_head. 
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
 */
------------------------------------------------------------------------------------------------------------------------------
/*
 * Circular doubly linked list implementation.
 *
 * Some of the internal functions ("__xxx") are useful when
 * manipulating whole lists rather than single entries, as
 * sometimes we already know the next/prev entries and we can
 * generate better code by using them directly rather than
 * using the generic single-entry routines.
 */
/*
* Реализация кольцевого двусвязного списка.
*
* Некоторые внутренние функции ("__xxx") полезны при
* манипулировании целыми списками, а не отдельными записями, так как
* иногда мы уже знаем следующие/предыдущие записи и можем
* сгенерировать лучший код, используя их напрямую, а не
* используя общие процедуры с одной записью.
*/
#define LIST_HEAD_INIT(name) { &(name), &(name) }                              // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371

#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)
/*
Новый список создается с помощью макроса LIST_HEAD(list_name). Он объявляет новую переменную с именем list_name, имеющую тип list_head, которая представляет собой пустой первый элемент, резервирующий место для головы нового списка. Кроме того, макрос инициализирует поля next и prev структуры list_head так, что переменная list_name указывает сама на себя.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/

------------------------------------------------------------------------------------------------------------------------------
/**
 * list_add - add a new entry
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
/**
* list_add - добавить новую запись
* @new: новая запись для добавления
* @head: заголовок списка, после которого нужно добавить
*
* Вставить новую запись после указанного заголовка.
* Это удобно для реализации стеков.
*/
/*
list_add(n,p) - Вставляет элемент, на который указывает n, сразу после элемента, на который указывает p. Чтобы вставить элемент n в начало списка, передайте адрес головы списка в качестве аргумента p.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/
static inline void list_add(struct list_head *new, struct list_head *head)       // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371       
{
	__list_add(new, head, head->next);
}

------------------------------------------------------------------------------------------------------------------------------
/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
/*
* Вставить новую запись между двумя известными последовательными записями.
*
* Это только для внутренних операций со списками, когда нам уже известны предыдущие и следующие записи!
*/
static inline void __list_add(struct list_head *new,                           // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371
			      struct list_head *prev,
			      struct list_head *next)
{
	if (!__list_add_valid(new, prev, next))
		return;

	next->prev = new;
	new->next = next;
	new->prev = prev;
	WRITE_ONCE(prev->next, new);
}

------------------------------------------------------------------------------------------------------------------------------
/**
 * list_add_tail - add a new entry
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
/**
* list_add_tail - добавить новую запись
* @new: новая запись для добавления
* @head: заголовок списка, перед которым нужно добавить
*
* Вставить новую запись перед указанным заголовком.
* Это полезно для реализации очередей.
*/
/*
Вставляет элемент, на который указывает n, непосредственно перед элементом, на который указывает p. Чтобы вставить элемент n в конец списка, передайте адрес головы списка в качестве аргумента p.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/
static inline void list_add_tail(struct list_head *new, struct list_head *head)           // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371
{
	__list_add(new, head->prev, head);
}

------------------------------------------------------------------------------------------------------------------------------
/**
 * list_del - deletes entry from list.
 * @entry: the element to delete from the list.
 * Note: list_empty() on entry does not return true after this, the entry is
 * in an undefined state.
 */
/**
* list_del — удаляет запись из списка.
* @entry: элемент, который необходимо удалить из списка.
* Примечание: list_empty() для записи не возвращает true, после этого запись находится в неопределённом состоянии.
*/
/*
Удаляет элемент, на который указывает p. Задавать голову списка нет необходимости.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/
static inline void list_del(struct list_head *entry)                                   // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371
{
	__list_del_entry(entry);
	entry->next = LIST_POISON1;
	entry->prev = LIST_POISON2;
}

------------------------------------------------------------------------------------------------------------------------------
/**
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
/**
* list_empty — проверяет, пуст ли список.
* @head: список для проверки.
*/
/*
Проверяет, пуст ли список, задаваемый адресом p головы списка.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/
static inline int list_empty(const struct list_head *head)                           // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371
{
	return READ_ONCE(head->next) == head;
}

------------------------------------------------------------------------------------------------------------------------------
/**
 * list_entry - get the struct for this entry
 * @ptr:	the &struct list_head pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 */
/**
* list_entry - получить структуру для этой записи
* @ptr: указатель на &struct list_head.
* @type: тип структуры, в которую встроен этот элемент.
* @member: имя list_head в структуре.
*/
/*
list_entry(p,t,m) - Возвращает адрес структуры типа t, включающей в себя поле типа list_head с именем m и адресом p.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/
#define list_entry(ptr, type, member) \                                             // https://elixir.bootlin.com/linux/v6.11/source/include/linux/list.h#L371
	container_of(ptr, type, member)
------------------------------------------------------------------------------------------------------------------------------
struct request_queue {
	/*
	 * The queue owner gets to use this for whatever they like.
	 * ll_rw_blk doesn't touch it.
	 */
	/*
	 * Владелец очереди может использовать это по своему усмотрению.
	 * ll_rw_blk не трогает это.
	 */
	void			*queuedata;

	struct elevator_queue	*elevator;

	const struct blk_mq_ops	*mq_ops;

	/* sw queues */
	struct blk_mq_ctx __percpu	*queue_ctx;

	/*
	 * various queue flags, see QUEUE_* below
	 */
	unsigned long		queue_flags;

	unsigned int		rq_timeout;

	unsigned int		queue_depth;

	refcount_t		refs;

	/* hw dispatch queues */
	unsigned int		nr_hw_queues;
	struct xarray		hctx_table;

	struct percpu_ref	q_usage_counter;

	struct request		*last_merge;

	spinlock_t		queue_lock;

	int			quiesce_depth;

	struct gendisk		*disk;

	/*
	 * mq queue kobject
	 */
	struct kobject *mq_kobj;

	struct queue_limits	limits;

#ifdef CONFIG_PM
	struct device		*dev;
	enum rpm_status		rpm_status;
#endif

	/*
	 * Number of contexts that have called blk_set_pm_only(). If this
	 * counter is above zero then only RQF_PM requests are processed.
	 */
	atomic_t		pm_only;

	struct blk_queue_stats	*stats;
	struct rq_qos		*rq_qos;
	struct mutex		rq_qos_mutex;

	/*
	 * ida allocated id for this queue.  Used to index queues from
	 * ioctx.
	 */
	int			id;

	/*
	 * queue settings
	 */
	unsigned long		nr_requests;	/* Max # of requests */

#ifdef CONFIG_BLK_INLINE_ENCRYPTION
	struct blk_crypto_profile *crypto_profile;
	struct kobject *crypto_kobject;
#endif

	struct timer_list	timeout;
	struct work_struct	timeout_work;

	atomic_t		nr_active_requests_shared_tags;

	struct blk_mq_tags	*sched_shared_tags;

	struct list_head	icq_list;
#ifdef CONFIG_BLK_CGROUP
	DECLARE_BITMAP		(blkcg_pols, BLKCG_MAX_POLS);
	struct blkcg_gq		*root_blkg;
	struct list_head	blkg_list;
	struct mutex		blkcg_mutex;
#endif

	int			node;

	spinlock_t		requeue_lock;
	struct list_head	requeue_list;
	struct delayed_work	requeue_work;

#ifdef CONFIG_BLK_DEV_IO_TRACE
	struct blk_trace __rcu	*blk_trace;
#endif
	/*
	 * for flush operations
	 */
	struct blk_flush_queue	*fq;
	struct list_head	flush_list;

	struct mutex		sysfs_lock;
	struct mutex		sysfs_dir_lock;
	struct mutex		limits_lock;

	/*
	 * for reusing dead hctx instance in case of updating
	 * nr_hw_queues
	 */
	struct list_head	unused_hctx_list;
	spinlock_t		unused_hctx_lock;

	int			mq_freeze_depth;

#ifdef CONFIG_BLK_DEV_THROTTLING
	/* Throttle data */
	struct throtl_data *td;
#endif
	struct rcu_head		rcu_head;
	wait_queue_head_t	mq_freeze_wq;
	/*
	 * Protect concurrent access to q_usage_counter by
	 * percpu_ref_kill() and percpu_ref_reinit().
	 */
	struct mutex		mq_freeze_lock;

	struct blk_mq_tag_set	*tag_set;
	struct list_head	tag_set_list;

	struct dentry		*debugfs_dir;
	struct dentry		*sched_debugfs_dir;
	struct dentry		*rqos_debugfs_dir;
	/*
	 * Serializes all debugfs metadata operations using the above dentries.
	 */
	struct mutex		debugfs_mutex;

	bool			mq_sysfs_init_done;
};

------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * each queue has an elevator_queue associated with it
 */
/*
 * каждая очередь имеет лифтовую_очередь, связанную с ней
 */
struct elevator_queue                                                                     // https://elixir.bootlin.com/linux/v6.11/source/block/elevator.h#L113
{
	struct elevator_type *type;
	void *elevator_data;
	struct kobject kobj;
	struct mutex sysfs_lock;
	unsigned long flags;
	DECLARE_HASHTABLE(hash, ELV_HASH_BITS);
};

------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * identifies an elevator type, such as AS or deadline
 */
/*
 * определяет тип лифта, например AS или deadline
 */
/*
 Вот краткое описание основных планировщиков I/O в Linux: 
 CFQ (Completely Fair Queuing):
 Распределяет время доступа к диску между процессами, стараясь обеспечить равномерный доступ для всех задач.
 Deadline scheduler:
 Устанавливает крайние сроки для выполнения операций ввода-вывода, что может быть полезно в системах реального времени или приложениях, чувствительных к задержкам.
 Anticipatory I/O scheduler:
 Предугадывает следующие операции ввода-вывода, пытаясь оптимизировать доступ к диску, но иногда это может приводить к увеличению задержек.
 Noop scheduler:
 Простой планировщик, который просто выполняет операции ввода-вывода в том порядке, в котором они поступают. Он эффективен для твердотельных накопителей (SSD) и устройств, где нет механических
 задержек.
*/ 
struct elevator_type                                                                      // https://elixir.bootlin.com/linux/v6.11/source/block/elevator.h#L64
{
	/* managed by elevator core */     /* управляется ядром лифта */
	struct kmem_cache *icq_cache;

	/* fields provided by elevator implementation */  /* поля, предоставляемые реализацией лифта */
	struct elevator_mq_ops ops;

	size_t icq_size;	/* see iocontext.h */
	size_t icq_align;	/* ditto */
	struct elv_fs_entry *elevator_attrs;
	const char *elevator_name;
	const char *elevator_alias;
	struct module *elevator_owner;
#ifdef CONFIG_BLK_DEBUG_FS
	const struct blk_mq_debugfs_attr *queue_debugfs_attrs;
	const struct blk_mq_debugfs_attr *hctx_debugfs_attrs;
#endif

	/* managed by elevator core */   /* управляется ядром лифта */
	char icq_cache_name[ELV_NAME_MAX + 6];	/* elvname + "_io_cq" */
	struct list_head list;
};

----------------------------------------------------------------------------------------------------------------------------------------------------------
struct elevator_mq_ops {                                                                   // https://elixir.bootlin.com/linux/v6.11/source/block/elevator.h#L26
	int (*init_sched)(struct request_queue *, struct elevator_type *);
	void (*exit_sched)(struct elevator_queue *);
	int (*init_hctx)(struct blk_mq_hw_ctx *, unsigned int);
	void (*exit_hctx)(struct blk_mq_hw_ctx *, unsigned int);
	void (*depth_updated)(struct blk_mq_hw_ctx *);

	bool (*allow_merge)(struct request_queue *, struct request *, struct bio *);
	bool (*bio_merge)(struct request_queue *, struct bio *, unsigned int);
	int (*request_merge)(struct request_queue *q, struct request **, struct bio *);
	void (*request_merged)(struct request_queue *, struct request *, enum elv_merge);
	void (*requests_merged)(struct request_queue *, struct request *, struct request *);
	void (*limit_depth)(blk_opf_t, struct blk_mq_alloc_data *);
	void (*prepare_request)(struct request *);
	void (*finish_request)(struct request *);
	void (*insert_requests)(struct blk_mq_hw_ctx *hctx, struct list_head *list,
			blk_insert_t flags);
	struct request *(*dispatch_request)(struct blk_mq_hw_ctx *);
	bool (*has_work)(struct blk_mq_hw_ctx *);
	void (*completed_request)(struct request *, u64);
	void (*requeue_request)(struct request *);
	struct request *(*former_request)(struct request_queue *, struct request *);
	struct request *(*next_request)(struct request_queue *, struct request *);
	void (*init_icq)(struct io_cq *);
	void (*exit_icq)(struct io_cq *);
};

----------------------------------------------------------------------------------------------------------------------------------------------------------
static struct elevator_type iosched_bfq_mq = {                                                // https://elixir.bootlin.com/linux/v6.11/source/block/bfq-iosched.c#L7611
	.ops = {
		.limit_depth		= bfq_limit_depth,
		.prepare_request	= bfq_prepare_request,
		.requeue_request        = bfq_finish_requeue_request,
		.finish_request		= bfq_finish_request,
		.exit_icq		= bfq_exit_icq,
		.insert_requests	= bfq_insert_requests,
		.dispatch_request	= bfq_dispatch_request,
		.next_request		= elv_rb_latter_request,
		.former_request		= elv_rb_former_request,
		.allow_merge		= bfq_allow_bio_merge,
		.bio_merge		= bfq_bio_merge,
		.request_merge		= bfq_request_merge,
		.requests_merged	= bfq_requests_merged,
		.request_merged		= bfq_request_merged,
		.has_work		= bfq_has_work,
		.depth_updated		= bfq_depth_updated,
		.init_hctx		= bfq_init_hctx,
		.init_sched		= bfq_init_queue,
		.exit_sched		= bfq_exit_queue,
	},

	.icq_size =		sizeof(struct bfq_io_cq),
	.icq_align =		__alignof__(struct bfq_io_cq),
	.elevator_attrs =	bfq_attrs,
	.elevator_name =	"bfq",
	.elevator_owner =	THIS_MODULE,
};

-----------------------------------------------------------------------------------------------------------------------------------------------------------
static int bfq_request_merge(struct request_queue *q, struct request **req,                      // https://elixir.bootlin.com/linux/v6.11/source/block/bfq-iosched.c#L2485
			     struct bio *bio)
{
	struct bfq_data *bfqd = q->elevator->elevator_data;
	struct request *__rq;

	__rq = bfq_find_rq_fmerge(bfqd, bio, q);
	if (__rq && elv_bio_merge_ok(__rq, bio)) {
		*req = __rq;

		if (blk_discard_mergable(__rq))
			return ELEVATOR_DISCARD_MERGE;
		return ELEVATOR_FRONT_MERGE;
	}

	return ELEVATOR_NO_MERGE;
}

----------------------------------------------------------------------------------------------------------------------------------------------------------
static int bfq_init_queue(struct request_queue *q, struct elevator_type *e)                        // https://elixir.bootlin.com/linux/v6.11/source/block/bfq-iosched.c#L7194
{
	struct bfq_data *bfqd;
	struct elevator_queue *eq;
	unsigned int i;
	struct blk_independent_access_ranges *ia_ranges = q->disk->ia_ranges;

	eq = elevator_alloc(q, e);
	if (!eq)
		return -ENOMEM;

	bfqd = kzalloc_node(sizeof(*bfqd), GFP_KERNEL, q->node);
	if (!bfqd) {
		kobject_put(&eq->kobj);
		return -ENOMEM;
	}
	eq->elevator_data = bfqd;

	spin_lock_irq(&q->queue_lock);
	q->elevator = eq;
	spin_unlock_irq(&q->queue_lock);

	/*
	 * Our fallback bfqq if bfq_find_alloc_queue() runs into OOM issues.
	 * Grab a permanent reference to it, so that the normal code flow
	 * will not attempt to free it.
	 * Set zero as actuator index: we will pretend that
	 * all I/O requests are for the same actuator.
	 */
	bfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0, 0);
	bfqd->oom_bfqq.ref++;
	bfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;
	bfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;
	bfqd->oom_bfqq.entity.new_weight =
		bfq_ioprio_to_weight(bfqd->oom_bfqq.new_ioprio);

	/* oom_bfqq does not participate to bursts */
	bfq_clear_bfqq_just_created(&bfqd->oom_bfqq);

	/*
	 * Trigger weight initialization, according to ioprio, at the
	 * oom_bfqq's first activation. The oom_bfqq's ioprio and ioprio
	 * class won't be changed any more.
	 */
	bfqd->oom_bfqq.entity.prio_changed = 1;

	bfqd->queue = q;

	bfqd->num_actuators = 1;
	/*
	 * If the disk supports multiple actuators, copy independent
	 * access ranges from the request queue structure.
	 */
	spin_lock_irq(&q->queue_lock);
	if (ia_ranges) {
		/*
		 * Check if the disk ia_ranges size exceeds the current bfq
		 * actuator limit.
		 */
		if (ia_ranges->nr_ia_ranges > BFQ_MAX_ACTUATORS) {
			pr_crit("nr_ia_ranges higher than act limit: iars=%d, max=%d.\n",
				ia_ranges->nr_ia_ranges, BFQ_MAX_ACTUATORS);
			pr_crit("Falling back to single actuator mode.\n");
		} else {
			bfqd->num_actuators = ia_ranges->nr_ia_ranges;

			for (i = 0; i < bfqd->num_actuators; i++) {
				bfqd->sector[i] = ia_ranges->ia_range[i].sector;
				bfqd->nr_sectors[i] =
					ia_ranges->ia_range[i].nr_sectors;
			}
		}
	}

	/* Otherwise use single-actuator dev info */
	if (bfqd->num_actuators == 1) {
		bfqd->sector[0] = 0;
		bfqd->nr_sectors[0] = get_capacity(q->disk);
	}
	spin_unlock_irq(&q->queue_lock);

	INIT_LIST_HEAD(&bfqd->dispatch);

	hrtimer_init(&bfqd->idle_slice_timer, CLOCK_MONOTONIC,
		     HRTIMER_MODE_REL);
	bfqd->idle_slice_timer.function = bfq_idle_slice_timer;

	bfqd->queue_weights_tree = RB_ROOT_CACHED;
#ifdef CONFIG_BFQ_GROUP_IOSCHED
	bfqd->num_groups_with_pending_reqs = 0;
#endif

	INIT_LIST_HEAD(&bfqd->active_list[0]);
	INIT_LIST_HEAD(&bfqd->active_list[1]);
	INIT_LIST_HEAD(&bfqd->idle_list);
	INIT_HLIST_HEAD(&bfqd->burst_list);

	bfqd->hw_tag = -1;
	bfqd->nonrot_with_queueing = blk_queue_nonrot(bfqd->queue);

	bfqd->bfq_max_budget = bfq_default_max_budget;

	bfqd->bfq_fifo_expire[0] = bfq_fifo_expire[0];
	bfqd->bfq_fifo_expire[1] = bfq_fifo_expire[1];
	bfqd->bfq_back_max = bfq_back_max;
	bfqd->bfq_back_penalty = bfq_back_penalty;
	bfqd->bfq_slice_idle = bfq_slice_idle;
	bfqd->bfq_timeout = bfq_timeout;

	bfqd->bfq_large_burst_thresh = 8;
	bfqd->bfq_burst_interval = msecs_to_jiffies(180);

	bfqd->low_latency = true;

	/*
	 * Trade-off between responsiveness and fairness.
	 */
	bfqd->bfq_wr_coeff = 30;
	bfqd->bfq_wr_rt_max_time = msecs_to_jiffies(300);
	bfqd->bfq_wr_min_idle_time = msecs_to_jiffies(2000);
	bfqd->bfq_wr_min_inter_arr_async = msecs_to_jiffies(500);
	bfqd->bfq_wr_max_softrt_rate = 7000; /*
					      * Approximate rate required
					      * to playback or record a
					      * high-definition compressed
					      * video.
					      */
	bfqd->wr_busy_queues = 0;

	/*
	 * Begin by assuming, optimistically, that the device peak
	 * rate is equal to 2/3 of the highest reference rate.
	 */
	bfqd->rate_dur_prod = ref_rate[blk_queue_nonrot(bfqd->queue)] *
		ref_wr_duration[blk_queue_nonrot(bfqd->queue)];
	bfqd->peak_rate = ref_rate[blk_queue_nonrot(bfqd->queue)] * 2 / 3;

	/* see comments on the definition of next field inside bfq_data */
	bfqd->actuator_load_threshold = 4;

	spin_lock_init(&bfqd->lock);

	/*
	 * The invocation of the next bfq_create_group_hierarchy
	 * function is the head of a chain of function calls
	 * (bfq_create_group_hierarchy->blkcg_activate_policy->
	 * blk_mq_freeze_queue) that may lead to the invocation of the
	 * has_work hook function. For this reason,
	 * bfq_create_group_hierarchy is invoked only after all
	 * scheduler data has been initialized, apart from the fields
	 * that can be initialized only after invoking
	 * bfq_create_group_hierarchy. This, in particular, enables
	 * has_work to correctly return false. Of course, to avoid
	 * other inconsistencies, the blk-mq stack must then refrain
	 * from invoking further scheduler hooks before this init
	 * function is finished.
	 */
	bfqd->root_group = bfq_create_group_hierarchy(bfqd, q->node);
	if (!bfqd->root_group)
		goto out_free;
	bfq_init_root_group(bfqd->root_group, bfqd);
	bfq_init_entity(&bfqd->oom_bfqq.entity, bfqd->root_group);

	/* We dispatch from request queue wide instead of hw queue */
	blk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);

	set_bit(ELEVATOR_FLAG_DISABLE_WBT, &eq->flags);
	wbt_disable_default(q->disk);
	blk_stat_enable_accounting(q);

	return 0;

out_free:
	kfree(bfqd);
	kobject_put(&eq->kobj);
	return -ENOMEM;
}

----------------------------------------------------------------------------------------------------------------------------------------------------------
static void bfq_exit_queue(struct elevator_queue *e)                                          // https://elixir.bootlin.com/linux/v6.11/source/block/bfq-iosched.c#L7140
{
	struct bfq_data *bfqd = e->elevator_data;
	struct bfq_queue *bfqq, *n;
	unsigned int actuator;

	hrtimer_cancel(&bfqd->idle_slice_timer);

	spin_lock_irq(&bfqd->lock);
	list_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)
		bfq_deactivate_bfqq(bfqd, bfqq, false, false);
	spin_unlock_irq(&bfqd->lock);

	for (actuator = 0; actuator < bfqd->num_actuators; actuator++)
		WARN_ON_ONCE(bfqd->rq_in_driver[actuator]);
	WARN_ON_ONCE(bfqd->tot_rq_in_driver);

	hrtimer_cancel(&bfqd->idle_slice_timer);

	/* release oom-queue reference to root group */   /* освободить ссылку oom-queue на корневую группу */
	bfqg_and_blkg_put(bfqd->root_group);

#ifdef CONFIG_BFQ_GROUP_IOSCHED
	blkcg_deactivate_policy(bfqd->queue->disk, &blkcg_policy_bfq);
#else
	spin_lock_irq(&bfqd->lock);
	bfq_put_async_queues(bfqd, bfqd->root_group);
	kfree(bfqd->root_group);
	spin_unlock_irq(&bfqd->lock);
#endif

	blk_stat_disable_accounting(bfqd->queue);
	clear_bit(ELEVATOR_FLAG_DISABLE_WBT, &e->flags);
	wbt_enable_default(bfqd->queue->disk);

	kfree(bfqd);
}

----------------------------------------------------------------------------------------------------------------------------------------------------------
static struct elevator_type mq_deadline = {                                        // https://elixir.bootlin.com/linux/v6.11/source/block/mq-deadline.c#L1077
	.ops = {
		.depth_updated		= dd_depth_updated,
		.limit_depth		= dd_limit_depth,
		.insert_requests	= dd_insert_requests,
		.dispatch_request	= dd_dispatch_request,
		.prepare_request	= dd_prepare_request,
		.finish_request		= dd_finish_request,
		.next_request		= elv_rb_latter_request,
		.former_request		= elv_rb_former_request,
		.bio_merge		= dd_bio_merge,
		.request_merge		= dd_request_merge,
		.requests_merged	= dd_merged_requests,
		.request_merged		= dd_request_merged,
		.has_work		= dd_has_work,
		.init_sched		= dd_init_sched,
		.exit_sched		= dd_exit_sched,
		.init_hctx		= dd_init_hctx,
	},

#ifdef CONFIG_BLK_DEBUG_FS
	.queue_debugfs_attrs = deadline_queue_debugfs_attrs,
#endif
	.elevator_attrs = deadline_attrs,
	.elevator_name = "mq-deadline",
	.elevator_alias = "deadline",
	.elevator_owner = THIS_MODULE,
};

----------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * Try to merge @bio into an existing request. If @bio has been merged into
 * an existing request, store the pointer to that request into *@rq.
 */
/*
* Попробуйте объединить @bio с существующим запросом. Если @bio был объединен с
* существующим запросом, сохраните указатель на этот запрос в *@rq.
*/
static int dd_request_merge(struct request_queue *q, struct request **rq,            // https://elixir.bootlin.com/linux/v6.11/source/block/mq-deadline.c#L621
			    struct bio *bio)
{
	struct deadline_data *dd = q->elevator->elevator_data;
	const u8 ioprio_class = IOPRIO_PRIO_CLASS(bio->bi_ioprio);
	const enum dd_prio prio = ioprio_class_to_prio[ioprio_class];
	struct dd_per_prio *per_prio = &dd->per_prio[prio];
	sector_t sector = bio_end_sector(bio);
	struct request *__rq;

	if (!dd->front_merges)
		return ELEVATOR_NO_MERGE;

	__rq = elv_rb_find(&per_prio->sort_list[bio_data_dir(bio)], sector);
	if (__rq) {
		BUG_ON(sector != blk_rq_pos(__rq));

		if (elv_bio_merge_ok(__rq, bio)) {
			*rq = __rq;
			if (blk_discard_mergable(__rq))
				return ELEVATOR_DISCARD_MERGE;
			return ELEVATOR_FRONT_MERGE;
		}
	}

	return ELEVATOR_NO_MERGE;
}

----------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * initialize elevator private data (deadline_data).
 */
/*
* инициализация личных данных лифта (deadline_data).
*/
static int dd_init_sched(struct request_queue *q, struct elevator_type *e)              // https://elixir.bootlin.com/linux/v6.11/source/block/mq-deadline.c#L571
{
	struct deadline_data *dd;
	struct elevator_queue *eq;
	enum dd_prio prio;
	int ret = -ENOMEM;

	eq = elevator_alloc(q, e);
	if (!eq)
		return ret;

	dd = kzalloc_node(sizeof(*dd), GFP_KERNEL, q->node);
	if (!dd)
		goto put_eq;

	eq->elevator_data = dd;

	for (prio = 0; prio <= DD_PRIO_MAX; prio++) {
		struct dd_per_prio *per_prio = &dd->per_prio[prio];

		INIT_LIST_HEAD(&per_prio->dispatch);
		INIT_LIST_HEAD(&per_prio->fifo_list[DD_READ]);
		INIT_LIST_HEAD(&per_prio->fifo_list[DD_WRITE]);
		per_prio->sort_list[DD_READ] = RB_ROOT;
		per_prio->sort_list[DD_WRITE] = RB_ROOT;
	}
	dd->fifo_expire[DD_READ] = read_expire;
	dd->fifo_expire[DD_WRITE] = write_expire;
	dd->writes_starved = writes_starved;
	dd->front_merges = 1;
	dd->last_dir = DD_WRITE;
	dd->fifo_batch = fifo_batch;
	dd->prio_aging_expire = prio_aging_expire;
	spin_lock_init(&dd->lock);

	/* We dispatch from request queue wide instead of hw queue */    /* Мы отправляем запросы из очереди запросов, а не из аппаратной очереди */
	blk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);

	q->elevator = eq;
	return 0;

put_eq:
	kobject_put(&eq->kobj);
	return ret;
}

----------------------------------------------------------------------------------------------------------------------------------------------------------
struct kobject {
	const char		*name;
	struct list_head	entry;
	struct kobject		*parent;
	struct kset		*kset;
	const struct kobj_type	*ktype;
	struct kernfs_node	*sd; /* sysfs directory entry */
	struct kref		kref;

	unsigned int state_initialized:1;
	unsigned int state_in_sysfs:1;
	unsigned int state_add_uevent_sent:1;
	unsigned int state_remove_uevent_sent:1;
	unsigned int uevent_suppress:1;

#ifdef CONFIG_DEBUG_KOBJECT_RELEASE
	struct delayed_work	release;
#endif
};

---------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * blk_rq_pos()			: the current sector
 */
/*
* blk_rq_pos() : текущий сектор
*/
static inline sector_t blk_rq_pos(const struct request *rq)                             // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L1065
{
	return rq->__sector;
}

---------------------------------------------------------------------------------------------------------------------------------------------------------
struct req_iterator {                                                                   // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L1036
	struct bvec_iter iter;
	struct bio *bio;
};
---------------------------------------------------------------------------------------------------------------------------------------------------------
#define rq_for_each_segment(bvl, _rq, _iter)			\                         // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L1045
	__rq_for_each_bio(_iter.bio, _rq)			\
		bio_for_each_segment(bvl, _iter.bio, _iter.iter)   
		
---------------------------------------------------------------------------------------------------------------------------------------------------------
#define __rq_for_each_bio(_bio, rq)	\                                                // https://elixir.bootlin.com/linux/v6.11/source/include/linux/blk-mq.h#L1041
	if ((rq->bio))			\
		for (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)  
		
---------------------------------------------------------------------------------------------------------------------------------------------------------
#define bio_for_each_segment(bvl, bio, iter)				\                 // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bio.h#L151
	__bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)
                   
---------------------------------------------------------------------------------------------------------------------------------------------------------
#define __bio_for_each_segment(bvl, bio, iter, start)			\         // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bio.h#L145
	for (iter = (start);						\
	     (iter).bi_size &&						\
		((bvl = bio_iter_iovec((bio), (iter))), 1);		\
	     bio_advance_iter_single((bio), &(iter), (bvl).bv_len))

---------------------------------------------------------------------------------------------------------------------------------------------------------
#define bio_iter_iovec(bio, iter)				\                        // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bio.h#L25
	bvec_iter_bvec((bio)->bi_io_vec, (iter))	
	
---------------------------------------------------------------------------------------------------------------------------------------------------------
#define bvec_iter_bvec(bvec, iter)				\                        // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L133
((struct bio_vec) {						\
	.bv_page	= bvec_iter_page((bvec), (iter)),	\
	.bv_len	= bvec_iter_len((bvec), (iter)),	\
	.bv_offset	= bvec_iter_offset((bvec), (iter)),	\
})    

---------------------------------------------------------------------------------------------------------------------------------------------------------
/* @bytes should be less or equal to bvec[i->bi_idx].bv_len */          /* @bytes должен быть меньше или равен bvec[i->bi_idx].bv_len */
static inline void bio_advance_iter_single(const struct bio *bio,                     // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bio.h#L111
					   struct bvec_iter *iter,
					   unsigned int bytes)
{
	iter->bi_sector += bytes >> 9;

	if (bio_no_advance_iter(bio))
		iter->bi_size -= bytes;
	else
		bvec_iter_advance_single(bio->bi_io_vec, iter, bytes);
}

----------------------------------------------------------------------------------------------------------------------------------------------------------
#define bvec_iter_page(bvec, iter)				\                       // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L129
	(mp_bvec_iter_page((bvec), (iter)) +			\
	 mp_bvec_iter_page_idx((bvec), (iter)))

----------------------------------------------------------------------------------------------------------------------------------------------------------
/* multi-page (mp_bvec) helpers */   /* многостраничные помощники (mp_bvec) */
#define mp_bvec_iter_page(bvec, iter)				\                      // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L101
	(__bvec_iter_bvec((bvec), (iter))->bv_page)
	
----------------------------------------------------------------------------------------------------------------------------------------------------------
/*
 * various member access, note that bio_data should of course not be used
 * on highmem page vectors
 */
/*
 * различный доступ к членам, обратите внимание, что bio_data, конечно, не следует использовать
 * на векторах страниц highmem
 */
#define __bvec_iter_bvec(bvec, iter)	(&(bvec)[(iter).bi_idx])                      // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L98

----------------------------------------------------------------------------------------------------------------------------------------------------------
#define mp_bvec_iter_page_idx(bvec, iter)			\                      // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L111
	(mp_bvec_iter_offset((bvec), (iter)) / PAGE_SIZE)
	
----------------------------------------------------------------------------------------------------------------------------------------------------------
#define mp_bvec_iter_offset(bvec, iter)				\              // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L108
	(__bvec_iter_bvec((bvec), (iter))->bv_offset + (iter).bi_bvec_done)

----------------------------------------------------------------------------------------------------------------------------------------------------------
#define bvec_iter_len(bvec, iter)				\                     // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L125
	min_t(unsigned, mp_bvec_iter_len((bvec), (iter)),		\
	      PAGE_SIZE - bvec_iter_offset((bvec), (iter)))
	     
----------------------------------------------------------------------------------------------------------------------------------------------------------
/**
 * min_t - return minimum of two values, using the specified type
 * @type: data type to use
 * @x: first value
 * @y: second value
 */
#define min_t(type, x, y) __cmp_once(min, type, x, y)                              // https://elixir.bootlin.com/linux/v6.11/source/include/linux/minmax.h#L213

----------------------------------------------------------------------------------------------------------------------------------------------------------
#define mp_bvec_iter_len(bvec, iter)				\                    // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L104
	min((iter).bi_size,					\
	    __bvec_iter_bvec((bvec), (iter))->bv_len - (iter).bi_bvec_done)

----------------------------------------------------------------------------------------------------------------------------------------------------------
/* For building single-page bvec in flight */   /* Для построения одностраничного bvec в полете */                           
 #define bvec_iter_offset(bvec, iter)				\                   // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L122
	(mp_bvec_iter_offset((bvec), (iter)) % PAGE_SIZE)
	
----------------------------------------------------------------------------------------------------------------------------------------------------------
#define mp_bvec_iter_offset(bvec, iter)				\           // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bvec.h#L108
	(__bvec_iter_bvec((bvec), (iter))->bv_offset + (iter).bi_bvec_done)
	

	
==========================================================================================================================================================
==========================================================================================================================================================
==========================================================================================================================================================
enum bh_state_bits {                                                            // https://elixir.bootlin.com/linux/v6.11/source/include/linux/buffer_head.h#L59
	BH_Uptodate,	/* Contains valid data */
	BH_Dirty,	/* Is dirty */
	BH_Lock,	/* Is locked */
	BH_Req,		/* Has been submitted for I/O */

	BH_Mapped,	/* Has a disk mapping */
	BH_New,		/* Disk mapping was newly created by get_block */
	BH_Async_Read,	/* Is under end_buffer_async_read I/O */
	BH_Async_Write,	/* Is under end_buffer_async_write I/O */
	BH_Delay,	/* Buffer is not yet allocated on disk */
	BH_Boundary,	/* Block is followed by a discontiguity */
	BH_Write_EIO,	/* I/O error on write */
	BH_Unwritten,	/* Buffer is allocated on disk but not written */
	BH_Quiet,	/* Buffer Error Prinks to be quiet */
	BH_Meta,	/* Buffer contains metadata */
	BH_Prio,	/* Buffer should be submitted with REQ_PRIO */
	BH_Defer_Completion, /* Defer AIO completion to workqueue */

	BH_PrivateStart,/* not a state bit, but the first bit available
			 * for private allocation by other entities
			 */
};

-----------------------------------------------------------------------------------------------------------------------------------------------------------
/*
Каждому блоку нужен собственный буфер. Это область оперативной памяти, в которой ядро сохраняет содержимое блока. Размер буфера блока всегда равен размеру самого блока.
У каждого буфера блока есть дескриптор "головы буфера", имеющий тип buffer_head. Этот дескриптор содержит всю информацию, необходимую ядру, чтобы знать, как обращаться с данным буфером. Т.о., перед работой с каждым буфером ядро проверяет его "голову".
Здесь рассотрим несколько полей срутруры buffer_head:
Поле b_page содержит адрес дескриптора страницы, для страничного кадра, в котором хранится буфер блока. Если страничный кадр находится в верхней памяти (что такое верхняя память????), поле b_data
содержит смещение буфера блока внутри страницы. В противном случае оно содержит начальный линейный адрес самого буфера блока. Поле b_bdev идентифицирует блочное устройство, использующее голову буфера.
(Daniel P. Bovet and Marco Cesati "Understanding the LINUX KERNEL THIRD EDITION")
*/  
/*
 * Historically, a buffer_head was used to map a single block
 * within a page, and of course as the unit of I/O through the
 * filesystem and block layers.  Nowadays the basic I/O unit
 * is the bio, and buffer_heads are used for extracting block
 * mappings (via a get_block_t call), for tracking state within
 * a folio (via a folio_mapping) and for wrapping bio submission
 * for backward compatibility reasons (e.g. submit_bh).
 */
/*
* Исторически buffer_head использовался для отображения одного блока
* на странице и, конечно, как единица ввода-вывода через
* файловую систему и слои блоков. В настоящее время базовой единицей ввода-вывода
* является bio, а buffer_heads используются для извлечения отображений блоков
* (через вызов get_block_t), для отслеживания состояния внутри
* фолио (через folio_mapping) и для упаковки отправки bio
* по соображениям обратной совместимости (например, submit_bh).
*/
struct buffer_head {                                                               // https://elixir.bootlin.com/linux/v6.11/source/include/linux/buffer_head.h#L59
	unsigned long b_state;		/* buffer state bitmap (see above) */   /* битовая карта состояния буфера (см. выше, т.е. enum bh_state_bits) */
	struct buffer_head *b_this_page;/* circular list of page's buffers */
	union {
		struct page *b_page;	/* the page this bh is mapped to */     /* страница, к которой привязан этот bh */
		struct folio *b_folio;	/* the folio this bh is mapped to */
	};

	sector_t b_blocknr;		/* start block number */                /* номер начального блока */
	size_t b_size;			/* size of mapping */
	char *b_data;			/* pointer to data within the page */   /* указатель на данные на странице */

	struct block_device *b_bdev;
	bh_end_io_t *b_end_io;		/* I/O completion */
 	void *b_private;		/* reserved for b_end_io */
	struct list_head b_assoc_buffers; /* associated with another mapping */
	struct address_space *b_assoc_map;	/* mapping this buffer is
						   associated with */
	atomic_t b_count;		/* users using this buffer_head */
	spinlock_t b_uptodate_lock;	/* Used by the first bh in a page, to
					 * serialise IO completion of other
					 * buffers in the page */
};

---------------------------------------------------------------------------------------------------------------------------------------------------
static inline struct bio *bio_alloc(struct block_device *bdev,                       // https://elixir.bootlin.com/linux/v6.11/source/include/linux/bio.h#L371
		unsigned short nr_vecs, blk_opf_t opf, gfp_t gfp_mask)
{
	return bio_alloc_bioset(bdev, nr_vecs, opf, gfp_mask, &fs_bio_set);
}

---------------------------------------------------------------------------------------------------------------------------------------------------
/**
 * bio_alloc_bioset - allocate a bio for I/O
 * @bdev:	block device to allocate the bio for (can be %NULL)
 * @nr_vecs:	number of bvecs to pre-allocate
 * @opf:	operation and flags for bio
 * @gfp_mask:   the GFP_* mask given to the slab allocator
 * @bs:		the bio_set to allocate from.
 *
 * Allocate a bio from the mempools in @bs.
 *
 * If %__GFP_DIRECT_RECLAIM is set then bio_alloc will always be able to
 * allocate a bio.  This is due to the mempool guarantees.  To make this work,
 * callers must never allocate more than 1 bio at a time from the general pool.
 * Callers that need to allocate more than 1 bio must always submit the
 * previously allocated bio for IO before attempting to allocate a new one.
 * Failure to do so can cause deadlocks under memory pressure.
 *
 * Note that when running under submit_bio_noacct() (i.e. any block driver),
 * bios are not submitted until after you return - see the code in
 * submit_bio_noacct() that converts recursion into iteration, to prevent
 * stack overflows.
 *
 * This would normally mean allocating multiple bios under submit_bio_noacct()
 * would be susceptible to deadlocks, but we have
 * deadlock avoidance code that resubmits any blocked bios from a rescuer
 * thread.
 *
 * However, we do not guarantee forward progress for allocations from other
 * mempools. Doing multiple allocations from the same mempool under
 * submit_bio_noacct() should be avoided - instead, use bio_set's front_pad
 * for per bio allocations.
 *
 * Returns: Pointer to new bio on success, NULL on failure.
 */
/** 
* bio_alloc_bioset - выделить биографию для ввода/вывода 
* @bdev: блокировать устройство для распределения биографии (может быть %NULL) 
* @nr_vecs: количество BVECs для предварительного выделения 
* @OPF: операция и флаги для биографии 
* @GFP_MASK: маска GFP_* 
* @bs: bio_set, чтобы выделить из. 
* 
* Выделите биографию от мемпул в @bs. 
* 
* Если %__ gfp_direct_reclaim установлен, bio_alloc всегда сможет 
* Выделите биографию. Это связано с гарантиями меморандумы. Чтобы сделать эту работу, 
* Абоненты никогда не должны выделять более 1 биографии за раз из общего бассейна. 
* Абоненты, которые должны выделить более 1 биографии, всегда должны отправлять 
* Ранее выделена биография для ИО, прежде чем попытаться выделить новую. 
* Неспособность сделать это может вызвать тупики под давлением памяти. 
* 
* Обратите внимание, что при запуске opper_bio_noacct () (то есть любой драйвер блока), 
* BIOS не отправляются до вернуться - см. Код в 
* отправить_bio_noacct (), который преобразует рекурсию в итерацию, чтобы предотвратить 
* Стокол переполнен. 
* 
* Это обычно означает выделение нескольких BIOS в соответствии с OPPOM_BIO_NOACCT () 
* будет восприимчива к тупикам, но у нас есть 
* Код избегания тупиков, который повторно повторно заблокирован BIOS от спасателя 
* нить. 
* 
* Тем не менее, мы не гарантируем прогресс на распределение от других 
* Мемпул. Выполняя множественные распределения из одного и того же мемпула под 
* opper_bio_noacct () следует избегать - вместо этого используйте bio_set's front_pad 
* для распределения биографии. 
* 
* Возвращает: указатель на новую биографию на успех, NULL при неудаче. 
*/
struct bio *bio_alloc_bioset(struct block_device *bdev, unsigned short nr_vecs,
			     blk_opf_t opf, gfp_t gfp_mask,
			     struct bio_set *bs)
{
	gfp_t saved_gfp = gfp_mask;
	struct bio *bio;
	void *p;

	/* should not use nobvec bioset for nr_vecs > 0 */
	if (WARN_ON_ONCE(!mempool_initialized(&bs->bvec_pool) && nr_vecs > 0))
		return NULL;

	if (opf & REQ_ALLOC_CACHE) {
		if (bs->cache && nr_vecs <= BIO_INLINE_VECS) {
			bio = bio_alloc_percpu_cache(bdev, nr_vecs, opf,
						     gfp_mask, bs);
			if (bio)
				return bio;
			/*
			 * No cached bio available, bio returned below marked with
			 * REQ_ALLOC_CACHE to particpate in per-cpu alloc cache.
			 */
		} else {
			opf &= ~REQ_ALLOC_CACHE;
		}
	}

	/*
	 * submit_bio_noacct() converts recursion to iteration; this means if
	 * we're running beneath it, any bios we allocate and submit will not be
	 * submitted (and thus freed) until after we return.
	 *
	 * This exposes us to a potential deadlock if we allocate multiple bios
	 * from the same bio_set() while running underneath submit_bio_noacct().
	 * If we were to allocate multiple bios (say a stacking block driver
	 * that was splitting bios), we would deadlock if we exhausted the
	 * mempool's reserve.
	 *
	 * We solve this, and guarantee forward progress, with a rescuer
	 * workqueue per bio_set. If we go to allocate and there are bios on
	 * current->bio_list, we first try the allocation without
	 * __GFP_DIRECT_RECLAIM; if that fails, we punt those bios we would be
	 * blocking to the rescuer workqueue before we retry with the original
	 * gfp_flags.
	 */
	/*
	 * submit_bio_noacct() преобразует рекурсию в итерацию; это означает, что если
	 * мы работаем под ним, любые биосы, которые мы выделяем и отправляем, не будут
	 * отправлены (и, таким образом, освобождены) до тех пор, пока мы не вернемся.
	 *
	 * Это подвергает нас потенциальной взаимоблокировке, если мы выделяем несколько биосов
	 * из одного и того же bio_set() во время работы под submit_bio_noacct().
	 * Если бы мы выделяли несколько биосов (например, драйвер стекового блока
	 *, который разделял биосы), мы бы заблокировались, если бы исчерпали
	 * резерв mempool.
	 *
	 * Мы решаем эту проблему и гарантируем прогресс вперед с помощью спасателя
	 * workqueue для каждого bio_set. Если мы переходим к выделению и есть биосы в
	 * current->bio_list, мы сначала пытаемся выделить без
	 * __GFP_DIRECT_RECLAIM; если это не удается, мы отправляем те биосы, которые мы
	 * блокируем, в рабочую очередь спасателя, прежде чем повторить попытку с оригинальными
	 * gfp_flags.
	 */
	if (current->bio_list &&
	    (!bio_list_empty(&current->bio_list[0]) ||
	     !bio_list_empty(&current->bio_list[1])) &&
	    bs->rescue_workqueue)
		gfp_mask &= ~__GFP_DIRECT_RECLAIM;

	p = mempool_alloc(&bs->bio_pool, gfp_mask);
	if (!p && gfp_mask != saved_gfp) {
		punt_bios_to_rescuer(bs);
		gfp_mask = saved_gfp;
		p = mempool_alloc(&bs->bio_pool, gfp_mask);
	}
	if (unlikely(!p))
		return NULL;
	if (!mempool_is_saturated(&bs->bio_pool))
		opf &= ~REQ_ALLOC_CACHE;

	bio = p + bs->front_pad;
	if (nr_vecs > BIO_INLINE_VECS) {
		struct bio_vec *bvl = NULL;

		bvl = bvec_alloc(&bs->bvec_pool, &nr_vecs, gfp_mask);
		if (!bvl && gfp_mask != saved_gfp) {
			punt_bios_to_rescuer(bs);
			gfp_mask = saved_gfp;
			bvl = bvec_alloc(&bs->bvec_pool, &nr_vecs, gfp_mask);
		}
		if (unlikely(!bvl))
			goto err_free;

		bio_init(bio, bdev, bvl, nr_vecs, opf);
	} else if (nr_vecs) {
		bio_init(bio, bdev, bio->bi_inline_vecs, BIO_INLINE_VECS, opf);
	} else {
		bio_init(bio, bdev, NULL, 0, opf);
	}

	bio->bi_pool = bs;
	return bio;

err_free:
	mempool_free(p, &bs->bio_pool);
	return NULL;
}
EXPORT_SYMBOL(bio_alloc_bioset);

